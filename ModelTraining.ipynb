{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0Auaxy1KZjWa","outputId":"cbca69f7-75a4-4d26-aaff-983836a750ac"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\surfy\\miniconda3\\envs\\MSCAI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\surfy\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     C:\\Users\\surfy\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import cv2\n","import pandas as pd\n","import re\n","import torch\n","import transformers\n","import os\n","\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_K_LWvCAFE9"},"outputs":[],"source":["googleColab = False\n","\n","if googleColab:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  # CHANGE THIS PATH IF USING COLAB\n","  %cd \"/content/drive/MyDrive/Msc Artificial Intelligence/Semester 1/Applied Artificial Intelligence/Assignment\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ypLqpsJ_Pys"},"outputs":[],"source":["# This is the model class used to outline the classifier models architecture\n","class Model(torch.nn.Module):\n","\n","  def __init__(self, miniLMInput=False):\n","    super().__init__()\n","\n","    # Changes the input size for the distill bert and mini lm embeddings\n","    inputSize = 768\n","\n","    if miniLMInput:\n","      inputSize = 384\n","\n","    # Defines the size and amount of linear layers the model has\n","    self.linear1 = torch.nn.Linear(inputSize, 128)\n","    self.linear2 = torch.nn.Linear(128, 64)\n","    self.linear3 = torch.nn.Linear(64, 32)\n","    # self.linear4 = torch.nn.Linear(256, 128)\n","    # self.linear5 = torch.nn.Linear(128, 30)\n","    # self.linear6 = torch.nn.Linear(30, 10)\n","    self.output = torch.nn.Linear(32, 1)\n","\n","    self.debug = False\n","\n","  def forward(self, x):\n","    # Runs through each layer with a forward pass and return output\n","    x = self.linear1(x)\n","    x = torch.nn.functional.leaky_relu(x)\n","\n","    if self.debug:\n","      print(f\"L1: {x}\")\n","\n","    x = self.linear2(x)\n","    x = torch.nn.functional.leaky_relu(x)\n","\n","    if self.debug:\n","      print(f\"L2: {x}\")\n","\n","    #drop = torch.nn.Dropout(p=0.7)\n","    #x = drop(self.linear3(x))\n","    x = self.linear3(x)\n","    x = torch.nn.functional.leaky_relu(x)\n","\n","    if self.debug:\n","      print(f\"L3: {x}\")\n","\n","    # drop = torch.nn.Dropout(p=0.3)\n","    # x = drop(self.linear4(x))\n","    # x = torch.nn.functional.relu(x)\n","\n","    # drop = torch.nn.Dropout(p=0.5)\n","    # x = drop(self.linear5(x))\n","    # x = torch.nn.functional.relu(x)\n","\n","    # drop = torch.nn.Dropout(p=0.7)\n","    # x = drop(self.linear6(x))\n","    # x = torch.nn.functional.relu(x)\n","\n","    x = self.output(x)\n","\n","    if self.debug:\n","      print(f\"x: {x}\")\n","\n","    return torch.nn.functional.sigmoid(x)\n","\n","  def setDebug(self, value):\n","    # sets debug to value\n","    self.debug = value"]},{"cell_type":"markdown","source":["##MINILM Training"],"metadata":{"id":"dCTHnZyyE9H7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8-xhcb9v7T6"},"outputs":[],"source":["# Gets mini lm test batches from files\n","aths = glob.glob(\"./datasetBothModels/miniLM/testbatches/*.npy\")\n","testBatches = [path for path in paths if \"labels\" not in path]\n","testLabels = [path for path in paths if \"labels\" in path]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80XJqOvq_Zq7"},"outputs":[],"source":["# Gets mini lm training batches from files\n","paths = glob.glob(\"./datasetBothModels/miniLM/batches/*.npy\")\n","batches = [path for path in paths if \"labels\" not in path]\n","labels = [path for path in paths if \"labels\" in path]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbBsccYD_NnK"},"outputs":[],"source":["# Sets the params for the model\n","params = {\n","    \"learningRate\": 0.001,\n","    \"optimizer\": \"Adam\"\n","}\n","\n","# Create the classifier model and sends it to the gpu for training\n","miniLMModel = Model(miniLMInput=True).cuda()\n","\n","# Defines the loss function (Binary Cross Entropy)\n","criterion = torch.nn.BCELoss()\n","\n","# Defines the loss optimizer\n","optimizer = torch.optim.Adam(miniLMModel.parameters(), lr = params[\"learningRate\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPb98QEhwEZE"},"outputs":[],"source":["def testModel():\n","  # This function runs the models on a validation dataset to assess acurracy\n","\n","  miniLMModel.eval()\n","\n","  predicted = []\n","  truth = []\n","\n","  # Runs through test batches and labels\n","  for batchPath, labelPath in zip(testBatches, testLabels):\n","\n","    # Loads the bathes into numpy arrays\n","    x2 = np.load(batchPath)\n","\n","    y2 = np.load(labelPath)\n","    y2 = y2.reshape(-1, 1)\n","\n","    # Splits the minilm batches into smaller batches of 256\n","    miniX = int(x2.shape[0] / 4)\n","\n","    for j in range(4):\n","      # Gets the 256 batches in seperate arrays\n","      idx = j*miniX\n","      y = torch.tensor(y2[idx:idx + miniX]).float()\n","      y = y.cuda()\n","\n","      x = torch.tensor(x2[idx:idx + miniX])\n","      x = x.cuda()\n","\n","      # Runs the models in evaluation mode on these batches\n","      with torch.no_grad():\n","        y_pred = miniLMModel(x)\n","\n","      y = y.cpu()\n","      y_pred = y_pred.cpu()\n","\n","      # Converts results to clamp to 0 or 1 to compare to the truth values\n","      y_pred = torch.where(y_pred <= 0.5,  0, 1)\n","      predicted = torch.cat((torch.tensor(predicted), y_pred))\n","      truth = torch.cat((torch.tensor(truth), y))\n","\n","  # Calculates accuracy\n","  acc = (truth == predicted).sum().float()/len(truth)\n","\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":875486,"status":"ok","timestamp":1702784511705,"user":{"displayName":"TOM HODGKINS","userId":"16752414918651802430"},"user_tz":0},"id":"gaY-0OH9_Jjv","outputId":"ea04b6d1-60c1-449c-a913-3a8c6099644f"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1204260450.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  predicted = torch.cat((torch.tensor(predicted), y_pred))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1204260450.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  truth = torch.cat((torch.tensor(truth), y))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1875591082.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  predicted = torch.cat((torch.tensor(predicted), y_pred))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1875591082.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  truth = torch.cat((torch.tensor(truth), y))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 | Loss 0.664980947971344 | Accuracy 0.6379973292350769 | Validation Accuracy 0.7190755009651184\n","Epoch 1 | Loss 0.6216778755187988 | Accuracy 0.6469676494598389 | Validation Accuracy 0.7438151240348816\n","Epoch 2 | Loss 0.6051216125488281 | Accuracy 0.6512933373451233 | Validation Accuracy 0.7445746660232544\n","Epoch 3 | Loss 0.6023092269897461 | Accuracy 0.6537482142448425 | Validation Accuracy 0.7379557490348816\n","Epoch 4 | Loss 0.5936565399169922 | Accuracy 0.6552110314369202 | Validation Accuracy 0.7418619990348816\n","Epoch 5 | Loss 0.5970460772514343 | Accuracy 0.6566124558448792 | Validation Accuracy 0.7345920205116272\n","Epoch 6 | Loss 0.5988903641700745 | Accuracy 0.6578119397163391 | Validation Accuracy 0.7293837070465088\n","Epoch 7 | Loss 0.605821967124939 | Accuracy 0.6590312123298645 | Validation Accuracy 0.6955295205116272\n","Epoch 8 | Loss 0.6273400783538818 | Accuracy 0.6603162288665771 | Validation Accuracy 0.6214192509651184\n","Epoch 9 | Loss 0.6338008642196655 | Accuracy 0.6613250970840454 | Validation Accuracy 0.5966796875\n","Epoch 10 | Loss 0.626792311668396 | Accuracy 0.6623830199241638 | Validation Accuracy 0.6129557490348816\n","Epoch 11 | Loss 0.620180070400238 | Accuracy 0.6636287569999695 | Validation Accuracy 0.62890625\n","Epoch 12 | Loss 0.6146676540374756 | Accuracy 0.6652365326881409 | Validation Accuracy 0.6427951455116272\n","Epoch 13 | Loss 0.607487142086029 | Accuracy 0.6671949625015259 | Validation Accuracy 0.6471354365348816\n","Epoch 14 | Loss 0.5950177907943726 | Accuracy 0.6692359447479248 | Validation Accuracy 0.6759982705116272\n","Epoch 15 | Loss 0.6118477582931519 | Accuracy 0.671420693397522 | Validation Accuracy 0.6418185830116272\n","Epoch 16 | Loss 0.6106752753257751 | Accuracy 0.6736889481544495 | Validation Accuracy 0.6385633945465088\n","Epoch 17 | Loss 0.6177787780761719 | Accuracy 0.6759557723999023 | Validation Accuracy 0.6170790195465088\n","Epoch 18 | Loss 0.6193188428878784 | Accuracy 0.6781953573226929 | Validation Accuracy 0.6128472089767456\n","Epoch 19 | Loss 0.6221129298210144 | Accuracy 0.6801974177360535 | Validation Accuracy 0.6088324785232544\n","Epoch 20 | Loss 0.6173286437988281 | Accuracy 0.681559145450592 | Validation Accuracy 0.6158854365348816\n","Epoch 21 | Loss 0.5936014652252197 | Accuracy 0.682245135307312 | Validation Accuracy 0.6693793535232544\n","Epoch 22 | Loss 0.5888105630874634 | Accuracy 0.6830251216888428 | Validation Accuracy 0.6571180820465088\n","Epoch 23 | Loss 0.5800266265869141 | Accuracy 0.6836163997650146 | Validation Accuracy 0.6761067509651184\n","Epoch 24 | Loss 0.5607705116271973 | Accuracy 0.6842207908630371 | Validation Accuracy 0.6769748330116272\n","Epoch 25 | Loss 0.5659946203231812 | Accuracy 0.6853281259536743 | Validation Accuracy 0.6697048544883728\n","Epoch 26 | Loss 0.561916708946228 | Accuracy 0.6866110563278198 | Validation Accuracy 0.6698133945465088\n","Epoch 27 | Loss 0.5538862347602844 | Accuracy 0.6880238652229309 | Validation Accuracy 0.6657986044883728\n","Epoch 28 | Loss 0.551624059677124 | Accuracy 0.689568042755127 | Validation Accuracy 0.6618923544883728\n","Epoch 29 | Loss 0.5417171716690063 | Accuracy 0.6910592317581177 | Validation Accuracy 0.6640625\n","Epoch 30 | Loss 0.5462875962257385 | Accuracy 0.6924325227737427 | Validation Accuracy 0.6552734375\n","Epoch 31 | Loss 0.5493910312652588 | Accuracy 0.6935868263244629 | Validation Accuracy 0.6526692509651184\n","Epoch 32 | Loss 0.5471537709236145 | Accuracy 0.6943579316139221 | Validation Accuracy 0.6625434160232544\n","Epoch 33 | Loss 0.537570595741272 | Accuracy 0.6953418254852295 | Validation Accuracy 0.6666666865348816\n","Epoch 34 | Loss 0.5323977470397949 | Accuracy 0.6964179277420044 | Validation Accuracy 0.6596137285232544\n","Epoch 35 | Loss 0.5197747945785522 | Accuracy 0.6976336240768433 | Validation Accuracy 0.6549479365348816\n","Epoch 36 | Loss 0.5159662961959839 | Accuracy 0.6989444494247437 | Validation Accuracy 0.6516926884651184\n","Epoch 37 | Loss 0.5003772377967834 | Accuracy 0.7003243565559387 | Validation Accuracy 0.6531032919883728\n","Epoch 38 | Loss 0.5008009672164917 | Accuracy 0.7017227411270142 | Validation Accuracy 0.6550564169883728\n","Epoch 39 | Loss 0.49550357460975647 | Accuracy 0.7031287550926208 | Validation Accuracy 0.6541883945465088\n","Epoch 40 | Loss 0.4918176233768463 | Accuracy 0.7045020461082458 | Validation Accuracy 0.6589626669883728\n","Epoch 41 | Loss 0.47877371311187744 | Accuracy 0.7058046460151672 | Validation Accuracy 0.6574435830116272\n","Epoch 42 | Loss 0.4869159460067749 | Accuracy 0.7069774270057678 | Validation Accuracy 0.6565755009651184\n","Epoch 43 | Loss 0.4758712351322174 | Accuracy 0.7080879211425781 | Validation Accuracy 0.6575520634651184\n","Epoch 44 | Loss 0.4766464829444885 | Accuracy 0.7091888785362244 | Validation Accuracy 0.6649305820465088\n","Epoch 45 | Loss 0.46755698323249817 | Accuracy 0.710241973400116 | Validation Accuracy 0.6682942509651184\n","Epoch 46 | Loss 0.4511162340641022 | Accuracy 0.711269736289978 | Validation Accuracy 0.6678602695465088\n","Epoch 47 | Loss 0.4493536055088043 | Accuracy 0.712263286113739 | Validation Accuracy 0.6675347089767456\n","Epoch 48 | Loss 0.4349057972431183 | Accuracy 0.7133800387382507 | Validation Accuracy 0.6691623330116272\n","Epoch 49 | Loss 0.4305025637149811 | Accuracy 0.7146005034446716 | Validation Accuracy 0.6637369990348816\n"]}],"source":["epochs = 50\n","\n","predicted = []\n","truth = []\n","epochLosses = []\n","\n","for i in range(epochs):\n","\n","  # Dynamic loss if\n","  # if len(losses) > 50:\n","  #   if losses[-1] <= 0.15:\n","  #     optimizer = torch.optim.Adam(miniLMModel.parameters(), lr = (params[\"learningRate\"] / 10))\n","\n","  losses = []\n","  miniLMModel.train()\n","  for batchPath, labelPath in zip(batches, labels):\n","\n","    # print(f\"batch: {batchPath[-17:]}\")\n","    # print(f\"batch: {labelPath[-17:]}\")\n","\n","    x2 = np.load(batchPath)\n","    #x = x / np.linalg.norm(x)\n","\n","    y2 = np.load(labelPath)\n","    y2 = y2.reshape(-1, 1)\n","    miniX = int(x2.shape[0] / 4)\n","\n","    for j in range(4):\n","\n","      idx = j*miniX\n","      y = torch.tensor(y2[idx:idx + miniX]).float()\n","      y = y.cuda()\n","\n","      x = torch.tensor(x2[idx:idx + miniX])\n","      x = x.cuda()\n","\n","      # Runs model of training batch\n","      y_pred = miniLMModel(x)\n","\n","      #print(f\"{x.size()} | {y.size()}\")\n","      #print(f\"yPred {y_pred}\")\n","      #print(f\"y {y}\")\n","      #print(y_pred)\n","      #print(y)\n","      #print(torch.max(y_pred, 1)[1])\n","      # print(f\"{y_pred.size()} | {y.size()}\")\n","      # print(f\"{y_pred.type()}) | {y.type()}\")\n","\n","      #print(y_pred.device)\n","      #print(y.device)\n","\n","      # Calculates loss based on difference between prediction and actual truth value\n","      loss = criterion(y_pred, y)\n","\n","      # Appends loss value for evaluation\n","      losses.append(loss.cpu().detach().numpy())\n","\n","      #print(f\"{y_pred.size()} | {y.size()}\")\n","      #print(f\"{y_pred[y_pred == 1]} \\n {y[:10]}\")\n","      #print(f\"{y_pred[y_pred == 0]}\")\n","      #print(f\"loss: {loss}\")\n","      #print((y==y_pred))\n","      #print(predicted)\n","      #print(y_pred)\n","\n","      # Takes optimizer step in loss space (updates model weights)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      y = y.cpu()\n","      y_pred = y_pred.cpu()\n","\n","      # Clamps y hat values to 0 or 1 for comparison with truth values\n","      y_pred = torch.where(y_pred <= 0.5,  0, 1)\n","      predicted = torch.cat((torch.tensor(predicted), y_pred))\n","      truth = torch.cat((torch.tensor(truth), y))\n","      # print(f\"y_pred: {y_pred.shape}\")\n","      # print(f\"y: {y.shape}\")\n","      # print(f\"predicted: {predicted.shape}\")\n","      # print(f\"truth: {truth.shape}\")\n","\n","  # Calculates training accuracy\n","  acc = (truth == predicted).sum().float()/len(truth)\n","\n","  # Validation test\n","  valAcc = testModel()\n","\n","  # Appends epochs average loss for evaluation\n","  epochLosses.append(np.array(losses).mean())\n","\n","  #losses = torch.cat((torch.tensor(losses), torch.tensor(loss.cpu().item())))\n","\n","  print(f\"Epoch {i} | Loss {loss.item()} | Accuracy {acc} | Validation Accuracy {valAcc}\")\n","\n","# Saves model with unique name to load later and evaluate\n","modelName = f\"MINILM-{round(acc.item(), 3)}-{round(valAcc.item(), 3)}-{round(epochLosses[-1].item(), 3)}-{epochs}-{params['learningRate']}-{params['optimizer']}\"\n","\n","folderPath = f\"./models/{modelName}/\"\n","\n","if not os.path.exists(folderPath):\n","    os.makedirs(folderPath)\n","\n","torch.save(miniLMModel, f\"{folderPath}{modelName}.pt\")\n","np.save(f\"{folderPath}/losses.npy\", np.array(epochLosses))"]},{"cell_type":"markdown","source":["##DistillBert Training"],"metadata":{"id":"W29Ne0rC82tj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_po-SWlA651J"},"outputs":[],"source":["# This code is the same as the mini lm code\n","\n","paths = glob.glob(\"./datasetBothModels/distillBert/testbatches/*.npy\")\n","testBatches = [path for path in paths if \"labels\" not in path]\n","testLabels = [path for path in paths if \"labels\" in path]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dl5lZ1Hm651J"},"outputs":[],"source":["paths = glob.glob(\"./datasetBothModels/distillBert/batches/*.npy\")\n","batches = [path for path in paths if \"labels\" not in path]\n","labels = [path for path in paths if \"labels\" in path]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7y32n7bF651J"},"outputs":[],"source":["params = {\n","    \"learningRate\": 0.01,\n","    \"optimizer\": \"Adamax\"\n","}\n","\n","distillModel = Model().cuda()\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.Adamax(distillModel.parameters(), lr = params[\"learningRate\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_z9riFc9651J"},"outputs":[],"source":["def testModelBert():\n","\n","  distillModel.eval()\n","\n","  predicted = []\n","  truth = []\n","\n","  for batchPath, labelPath in zip(testBatches, testLabels):\n","\n","    x2 = np.load(batchPath)\n","\n","    y2 = np.load(labelPath)\n","    y2 = y2.reshape(-1, 1)\n","\n","    y = torch.tensor(y2).float()\n","    y = y.cuda()\n","\n","    x = torch.tensor(x2)\n","    x = x.cuda()\n","\n","    with torch.no_grad():\n","      y_pred = distillModel(x)\n","\n","    y = y.cpu()\n","    y_pred = y_pred.cpu()\n","\n","    y_pred = torch.where(y_pred <= 0.5,  0, 1)\n","    predicted = torch.cat((torch.tensor(predicted), y_pred))\n","    truth = torch.cat((torch.tensor(truth), y))\n","\n","  acc = (truth == predicted).sum().float()/len(truth)\n","\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELmQqsLe651J","outputId":"ff239a7c-beda-4ac6-9e7b-a15f79bc8f6b"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1084828420.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  predicted = torch.cat((torch.tensor(predicted), y_pred))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\1084828420.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  truth = torch.cat((torch.tensor(truth), y))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\3311949564.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  predicted = torch.cat((torch.tensor(predicted), y_pred))\n","C:\\Users\\surfy\\AppData\\Local\\Temp\\ipykernel_24256\\3311949564.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  truth = torch.cat((torch.tensor(truth), y))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 | Loss 0.45294272899627686 | Accuracy 0.7359274625778198 | Validation Accuracy 0.6012369990348816\n","Epoch 1 | Loss 0.4427000880241394 | Accuracy 0.747971773147583 | Validation Accuracy 0.6047092080116272\n","Epoch 2 | Loss 0.4259592890739441 | Accuracy 0.7533921003341675 | Validation Accuracy 0.6073133945465088\n","Epoch 3 | Loss 0.39586180448532104 | Accuracy 0.7570412755012512 | Validation Accuracy 0.6057942509651184\n","Epoch 4 | Loss 0.41417789459228516 | Accuracy 0.7600300312042236 | Validation Accuracy 0.6064453125\n","Epoch 5 | Loss 0.4263767600059509 | Accuracy 0.7623080015182495 | Validation Accuracy 0.6048176884651184\n","Epoch 6 | Loss 0.41418543457984924 | Accuracy 0.7642327547073364 | Validation Accuracy 0.6115451455116272\n","Epoch 7 | Loss 0.4214744567871094 | Accuracy 0.7659767866134644 | Validation Accuracy 0.6117621660232544\n","Epoch 8 | Loss 0.3938171863555908 | Accuracy 0.7673934102058411 | Validation Accuracy 0.6116536259651184\n","Epoch 9 | Loss 0.40710151195526123 | Accuracy 0.768660843372345 | Validation Accuracy 0.6103515625\n","Epoch 10 | Loss 0.4081365764141083 | Accuracy 0.7699528336524963 | Validation Accuracy 0.6092665195465088\n","Epoch 11 | Loss 0.3858085572719574 | Accuracy 0.7711672186851501 | Validation Accuracy 0.6112196445465088\n","Epoch 12 | Loss 0.3918673098087311 | Accuracy 0.7722394466400146 | Validation Accuracy 0.6079643964767456\n","Epoch 13 | Loss 0.4098106622695923 | Accuracy 0.7731584906578064 | Validation Accuracy 0.6159939169883728\n","Epoch 14 | Loss 0.37835249304771423 | Accuracy 0.7740798592567444 | Validation Accuracy 0.6103515625\n","Epoch 15 | Loss 0.39967697858810425 | Accuracy 0.7750306725502014 | Validation Accuracy 0.6137152910232544\n","Epoch 16 | Loss 0.3864707946777344 | Accuracy 0.7758896350860596 | Validation Accuracy 0.6139323115348816\n","Epoch 17 | Loss 0.3834052085876465 | Accuracy 0.7767322063446045 | Validation Accuracy 0.6140407919883728\n","Epoch 18 | Loss 0.37176015973091125 | Accuracy 0.7775266766548157 | Validation Accuracy 0.6102430820465088\n","Epoch 19 | Loss 0.4009738564491272 | Accuracy 0.7782521843910217 | Validation Accuracy 0.6126301884651184\n","Epoch 20 | Loss 0.37992680072784424 | Accuracy 0.778915286064148 | Validation Accuracy 0.6148003339767456\n","Epoch 21 | Loss 0.38078296184539795 | Accuracy 0.7796173691749573 | Validation Accuracy 0.6112196445465088\n","Epoch 22 | Loss 0.38091450929641724 | Accuracy 0.7803019881248474 | Validation Accuracy 0.6131727695465088\n","Epoch 23 | Loss 0.3959903120994568 | Accuracy 0.7809207439422607 | Validation Accuracy 0.6146918535232544\n","Epoch 24 | Loss 0.3888775110244751 | Accuracy 0.781493604183197 | Validation Accuracy 0.6075303554534912\n","Epoch 25 | Loss 0.38277900218963623 | Accuracy 0.782048225402832 | Validation Accuracy 0.6092665195465088\n","Epoch 26 | Loss 0.3852333128452301 | Accuracy 0.7826303839683533 | Validation Accuracy 0.6137152910232544\n","Epoch 27 | Loss 0.3778720498085022 | Accuracy 0.7831805944442749 | Validation Accuracy 0.6105685830116272\n","Epoch 28 | Loss 0.41590186953544617 | Accuracy 0.7837153077125549 | Validation Accuracy 0.6126301884651184\n","Epoch 29 | Loss 0.3862432539463043 | Accuracy 0.784241795539856 | Validation Accuracy 0.6141493320465088\n","Epoch 30 | Loss 0.38474857807159424 | Accuracy 0.7847297787666321 | Validation Accuracy 0.6112196445465088\n","Epoch 31 | Loss 0.38917359709739685 | Accuracy 0.7852082252502441 | Validation Accuracy 0.6127387285232544\n","Epoch 32 | Loss 0.37413328886032104 | Accuracy 0.785675585269928 | Validation Accuracy 0.6141493320465088\n","Epoch 33 | Loss 0.37473684549331665 | Accuracy 0.7861634492874146 | Validation Accuracy 0.6110026240348816\n","Epoch 34 | Loss 0.37271320819854736 | Accuracy 0.7866054177284241 | Validation Accuracy 0.6098090410232544\n","Epoch 35 | Loss 0.36824122071266174 | Accuracy 0.7870793342590332 | Validation Accuracy 0.6124131679534912\n","Epoch 36 | Loss 0.38761985301971436 | Accuracy 0.7875517010688782 | Validation Accuracy 0.6130642294883728\n","Epoch 37 | Loss 0.37769198417663574 | Accuracy 0.7879749536514282 | Validation Accuracy 0.6097005009651184\n","Epoch 38 | Loss 0.37604087591171265 | Accuracy 0.788409411907196 | Validation Accuracy 0.6119791865348816\n","Epoch 39 | Loss 0.3861677050590515 | Accuracy 0.7888343930244446 | Validation Accuracy 0.6114366054534912\n","Epoch 40 | Loss 0.3452548384666443 | Accuracy 0.7892274260520935 | Validation Accuracy 0.6130642294883728\n","Epoch 41 | Loss 0.3540373146533966 | Accuracy 0.7896190881729126 | Validation Accuracy 0.6140407919883728\n","Epoch 42 | Loss 0.389533132314682 | Accuracy 0.7899648547172546 | Validation Accuracy 0.6103515625\n","Epoch 43 | Loss 0.36087676882743835 | Accuracy 0.790378212928772 | Validation Accuracy 0.6116536259651184\n","Epoch 44 | Loss 0.3832351565361023 | Accuracy 0.7907480597496033 | Validation Accuracy 0.6129557490348816\n","Epoch 45 | Loss 0.35447365045547485 | Accuracy 0.7910903096199036 | Validation Accuracy 0.6133897304534912\n","Epoch 46 | Loss 0.3664122223854065 | Accuracy 0.7914296984672546 | Validation Accuracy 0.6112196445465088\n","Epoch 47 | Loss 0.3691917061805725 | Accuracy 0.7917795777320862 | Validation Accuracy 0.6083984375\n","Epoch 48 | Loss 0.34630286693573 | Accuracy 0.7921386361122131 | Validation Accuracy 0.6106770634651184\n","Epoch 49 | Loss 0.3578423857688904 | Accuracy 0.7924789786338806 | Validation Accuracy 0.6121962070465088\n"]}],"source":["epochs = 50\n","\n","predicted = []\n","truth = []\n","epochLosses = []\n","\n","for i in range(epochs):\n","\n","  # if len(losses) > 50:\n","  #   if losses[-1] <= 0.15:\n","  #     optimizer = torch.optim.Adam(distillModel.parameters(), lr = (params[\"learningRate\"] / 10))\n","\n","  losses = []\n","  distillModel.train()\n","  for batchPath, labelPath in zip(batches, labels):\n","    # Batches are no longer split into 4 as distill bert was saved in batches of 256 already\n","\n","    # print(f\"batch: {batchPath[-17:]}\")\n","    # print(f\"batch: {labelPath[-17:]}\")\n","\n","    x2 = np.load(batchPath)\n","    #x = x / np.linalg.norm(x)\n","\n","    y2 = np.load(labelPath)\n","    y2 = y2.reshape(-1, 1)\n","\n","    y = torch.tensor(y2).float()\n","    y = y.cuda()\n","\n","    x = torch.tensor(x2)\n","    x = x.cuda()\n","\n","    y_pred = distillModel(x)\n","\n","    #print(f\"{x.size()} | {y.size()}\")\n","    #print(f\"yPred {y_pred}\")\n","    #print(f\"y {y}\")\n","    #print(y_pred)\n","    #print(y)\n","    #print(torch.max(y_pred, 1)[1])\n","    # print(f\"{y_pred.size()} | {y.size()}\")\n","    # print(f\"{y_pred.type()}) | {y.type()}\")\n","\n","    #print(y_pred.device)\n","    #print(y.device)\n","\n","    loss = criterion(y_pred, y)\n","\n","    losses.append(loss.cpu().detach().numpy())\n","\n","    #print(f\"{y_pred.size()} | {y.size()}\")\n","    #print(f\"{y_pred[y_pred == 1]} \\n {y[:10]}\")\n","    #print(f\"{y_pred[y_pred == 0]}\")\n","    #print(f\"loss: {loss}\")\n","    #print((y==y_pred))\n","    #print(predicted)\n","    #print(y_pred)\n","\n","    #loss = loss.cpu()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    y = y.cpu()\n","    y_pred = y_pred.cpu()\n","\n","    y_pred = torch.where(y_pred <= 0.5,  0, 1)\n","    predicted = torch.cat((torch.tensor(predicted), y_pred))\n","    truth = torch.cat((torch.tensor(truth), y))\n","    # print(f\"y_pred: {y_pred.shape}\")\n","    # print(f\"y: {y.shape}\")\n","    # print(f\"predicted: {predicted.shape}\")\n","    # print(f\"truth: {truth.shape}\")\n","\n","  acc = (truth == predicted).sum().float()/len(truth)\n","\n","  valAcc = testModelBert()\n","\n","  epochLosses.append(np.array(losses).mean())\n","\n","  #losses = torch.cat((torch.tensor(losses), torch.tensor(loss.cpu().item())))\n","\n","  print(f\"Epoch {i} | Loss {loss.item()} | Accuracy {acc} | Validation Accuracy {valAcc}\")\n","\n","modelName = f\"DISTILLBERT-{round(acc.item(), 3)}-{round(valAcc.item(), 3)}-{round(epochLosses[-1].item(), 3)}-{epochs}-{params['learningRate']}-{params['optimizer']}\"\n","\n","folderPath = f\"./models/{modelName}/\"\n","\n","if not os.path.exists(folderPath):\n","    os.makedirs(folderPath)\n","\n","torch.save(distillModel, f\"{folderPath}{modelName}.pt\")\n","np.save(f\"{folderPath}/losses.npy\", np.array(epochLosses))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}