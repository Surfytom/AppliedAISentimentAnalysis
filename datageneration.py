# -*- coding: utf-8 -*-
"""DataGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yV8_P-e0CNKFPm0LDBD_5Dt33Fch6uv
"""

import numpy as np
import matplotlib.pyplot as plt
import glob
import cv2
import pandas as pd
import re
import torch
import transformers

import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

# Loads the twitter dataset into pandas dataframe
dataframe = pd.read_csv("./Dataset/training.csv",
                        encoding='ISO-8859-1',
                        names=['target','ids','date','flag','user','tweet'])

# Changes the target value for postive from 4 to 1 so it is more compatible with binary classification
dataframe['target'] = np.where(dataframe['target'] == 4, 1, 0)

# Fractions 100,000 data points from the dataframe
dataframe = dataframe.sample(frac = 0.0625, random_state = 2)

dataframe

"""##Pre-Processing"""

from textPreProcessing import DataProcessor
# Pre-processes the text using my .py script
cleaner = DataProcessor()

dataframe["tweet"] = cleaner.CleanTextData(dataframe["tweet"])

"""##Transformer Embedding Generation

### Distill Bert
"""

# Loads the distill bert model and tokenizer from hugging face to use for embedding generation
modelClass, tokenizerClass, pretrainedWeights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')

distillTokenizer = tokenizerClass.from_pretrained(pretrainedWeights)
distillModel = modelClass.from_pretrained(pretrainedWeights)

# Tokenizes the tweets
distillTokenized = dataframe['tweet'].apply(lambda x: distillTokenizer.encode(x, add_special_tokens=True, truncation=True, padding='max_length', max_length=512))

# Assigns the tokenized tweets as inputs and attention masks for the tranformer
input_ids = torch.tensor(distillTokenized.values.tolist())
attention_mask = torch.tensor(np.where(input_ids == 0, input_ids, 1))
labels = dataframe["target"].values.tolist()

def runOneInferenceDistill(inputIds, attentionMask, labels, fileCount):

  # Sends the inputIds and attention mask to the GPU
  inputIds = inputIds.cuda()
  attentionMask = attentionMask.cuda()

  # Runs the distillbert model in evaluation mode and grabs the resulting CLS tokens
  distillModel.eval()
  with torch.no_grad():
    clsTokens = distillModel(inputIds, attention_mask=attentionMask)[0][:, 0, :].cpu().numpy()

  # Saves the labels and batch to a folder
  np.save(f"./datasetBothModels/distillBert/batches/batch{fileCount}.npy",
           clsTokens)

  np.save(f"./datasetBothModels/distillBert/batches/batch{fileCount}labels.npy",
          labels)

  clsTokens = None

# Loops round the 100,000 data points in increments of 256 batches generating word embeddings
batchSize = 256
distillModel = distillModel.cuda()
for i in range(int(input_ids.shape[0]/batchSize)):

  runOneInferenceDistill(input_ids[i*batchSize : (i+1)*batchSize], attention_mask[i*batchSize : (i+1)*batchSize], labels[i*batchSize : (i+1)*batchSize], i)

"""### MINILM"""

# Imports the miniLM model and tokenizer from hugging face

miniLMTokenizer = transformers.AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')
miniLMModel = transformers.AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')

# Grabs the labels from the dataset
labels = dataframe["target"].values.tolist()

def runOneInferenceMiniLM(inputs, labels, fileCount):

  # Runs the mini lm model in evaluation mode and grabs the cls tokens
  miniLMModel.eval()
  with torch.no_grad():
    clsTokens = miniLMModel(**inputs)[0][:, 0, :].cpu().numpy()

  # Saves the word embeddings and labels
  np.save(f"./datasetBothModels/miniLM/batches/batch{fileCount}.npy",
           clsTokens)

  np.save(f"./datasetBothModels/miniLM/batches/batch{fileCount}labels.npy",
          labels)

  clsTokens = None

# Iterates over the dataset in batches of 1024 saving the outputs
batchSize = 1024
miniLMModel = miniLMModel.cuda()

# converts the tweets to a list for the tokenizer
data = dataframe['tweet'].tolist()

for i in range(int(100000/batchSize)):
  # Tokenizes thet tweets and send them to the gpu
  inputs = miniLMTokenizer(data[i*batchSize : (i+1)*batchSize], add_special_tokens=True, truncation=True, padding='max_length', max_length=256, return_tensors='pt').to('cuda')
  runOneInferenceMiniLM(inputs, labels[i*batchSize : (i+1)*batchSize], i)